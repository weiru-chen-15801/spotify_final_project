{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pylast\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66, 30, 13, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_features = []\n",
    "valid_data_indices = []\n",
    "for i in range(0, 100):\n",
    "    feature_sing = pd.read_csv(\"./playlists/df_playlist_features_\" + str(i) + \".csv\")\n",
    "    feature_sing = feature_sing.drop(columns=['track_name', 'artist', 'id','Unnamed: 0'])\n",
    "    feature_sing = feature_sing.dropna()\n",
    "#     print(feature_sing.tail())\n",
    "    feature_sing = feature_sing.to_numpy()\n",
    "    num_songs = feature_sing.shape[0]\n",
    "    if (num_songs >= 30):\n",
    "        valid_data_indices.append(i)\n",
    "        playlist_features.append(feature_sing[:30, ].reshape((30, 13, 1)))\n",
    "playlist_features = np.array(playlist_features)\n",
    "print(len(valid_data_indices))\n",
    "playlist_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "playlist_features_mean = np.mean(playlist_features, axis=(0, 1), keepdims = True)\n",
    "playlist_features_std = np.std(playlist_features, axis=(0, 1), keepdims = True)\n",
    "playlist_features = (playlist_features - playlist_features_mean) / playlist_features_std\n",
    "# playlist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist_features_mean = np.mean(playlist_features, axis=(0), keepdims = True)\n",
    "# playlist_features_std = np.std(playlist_features, axis=(0), keepdims = True)\n",
    "# playlist_features = (playlist_features - playlist_features_mean) / playlist_features_std\n",
    "# playlist_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in labels\n",
    "playlist_tags = []\n",
    "with open('labels_100_playlists.json') as json_file:\n",
    "    raw_label = json.load(json_file)\n",
    "    for i in range(len(raw_label)):\n",
    "        # filter labels with valid features\n",
    "        if i in valid_data_indices:\n",
    "            label = raw_label[i]\n",
    "            label = np.array(list(label.values()))\n",
    "#             print(label)\n",
    "#             label = label.reshape((-1, 1))\n",
    "            playlist_tags.append(label)\n",
    "\n",
    "    playlist_tags = np.array(playlist_tags)\n",
    "#     print(playlist_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 21, 36, 5, 41, 43, 11]\n"
     ]
    }
   ],
   "source": [
    "# train-test split get random indices\n",
    "random.seed( 209 )\n",
    "test_indices = random.sample(range(0, 66), 7)\n",
    "print(test_indices)\n",
    "\n",
    "# split features\n",
    "train_features = []\n",
    "test_features = []\n",
    "for i in range(len(playlist_features)):\n",
    "    if i in test_indices:\n",
    "        test_features.append(playlist_features[i])\n",
    "    else:\n",
    "        train_features.append(playlist_features[i])\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "# split labels\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "for i in range(len(playlist_tags)):\n",
    "    if i in test_indices:\n",
    "        test_labels.append(playlist_tags[i])\n",
    "    else:\n",
    "        train_labels.append(playlist_tags[i])\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_features[0].shape\n",
    "num_category = train_labels[0].shape[0]\n",
    "\n",
    "##model building\n",
    "model = models.Sequential()\n",
    "#convolutional layer with rectified linear unit activation\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#64 convolution filters used each of size 3x3\n",
    "#choose the best features via pooling\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "#randomly turn neurons on and off to improve convergence\n",
    "# model.add(layers.Dropout(0.25))\n",
    "#flatten since too many dimensions, we only want a classification output\n",
    "model.add(layers.Flatten())\n",
    "#fully connected to get all relevant data\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "#one more dropout for convergence' sake :) \n",
    "model.add(layers.Dropout(0.2))\n",
    "#output a softmax to squash the matrix into output probabilities\n",
    "model.add(layers.Dense(num_category, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build CNN structure\n",
    "# input_shape = train_features[0].shape\n",
    "# num_category = train_labels[0].shape[0]\n",
    "\n",
    "# # model building\n",
    "# model = models.Sequential()\n",
    "# #convolutional layer with rectified linear unit activation\n",
    "# model.add(layers.Conv2D(32, kernel_size=(3, 3),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape))\n",
    "# #32 convolution filters with each size 3x3\n",
    "# # model.add(layers.Conv2D(1024, (3, 3), activation='relu'))\n",
    "# #64 convolution filters used each of size 3x3\n",
    "# #choose the best features by adding a pooling layer\n",
    "# # model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "# #randomly turn neurons on and off to improve convergence\n",
    "# model.add(layers.Dropout(0.25))\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# #64 convolution filters used each of size 3x3\n",
    "# #choose the best features by adding a pooling layer\n",
    "# # model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(layers.Dropout(0.25))\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "# #64 convolution filters used each of size 3x3\n",
    "# #choose the best features by adding a pooling layer\n",
    "# # model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "# # model.add(layers.Dropout(0.25))\n",
    "# #flatten layer\n",
    "# model.add(layers.Flatten())\n",
    "# #fully connected to get all relevant data\n",
    "# model.add(layers.Dense(128, activation='relu'))\n",
    "# #one more dropout for convergence\n",
    "# # model.add(layers.Dropout(0.5))\n",
    "# #output multiple sigmoid to generate output probabilities\n",
    "# model.add(layers.Dense(num_category, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use adaDelta as the adaptive learning rate\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59 samples, validate on 7 samples\n",
      "Epoch 1/30\n",
      "59/59 [==============================] - 0s 881us/sample - loss: 0.0477 - accuracy: 0.4853 - val_loss: 0.0388 - val_accuracy: 0.4896\n",
      "Epoch 2/30\n",
      "59/59 [==============================] - 0s 814us/sample - loss: 0.0469 - accuracy: 0.4853 - val_loss: 0.0389 - val_accuracy: 0.4896\n",
      "Epoch 3/30\n",
      "59/59 [==============================] - 0s 832us/sample - loss: 0.0467 - accuracy: 0.4853 - val_loss: 0.0389 - val_accuracy: 0.4896\n",
      "Epoch 4/30\n",
      "59/59 [==============================] - 0s 787us/sample - loss: 0.0484 - accuracy: 0.4853 - val_loss: 0.0387 - val_accuracy: 0.4896\n",
      "Epoch 5/30\n",
      "59/59 [==============================] - 0s 770us/sample - loss: 0.0473 - accuracy: 0.4853 - val_loss: 0.0385 - val_accuracy: 0.4896\n",
      "Epoch 6/30\n",
      "59/59 [==============================] - 0s 723us/sample - loss: 0.0472 - accuracy: 0.4853 - val_loss: 0.0383 - val_accuracy: 0.4896\n",
      "Epoch 7/30\n",
      "59/59 [==============================] - 0s 720us/sample - loss: 0.0467 - accuracy: 0.4853 - val_loss: 0.0381 - val_accuracy: 0.4896\n",
      "Epoch 8/30\n",
      "59/59 [==============================] - 0s 728us/sample - loss: 0.0467 - accuracy: 0.4852 - val_loss: 0.0380 - val_accuracy: 0.4896\n",
      "Epoch 9/30\n",
      "59/59 [==============================] - 0s 670us/sample - loss: 0.0474 - accuracy: 0.4853 - val_loss: 0.0380 - val_accuracy: 0.4896\n",
      "Epoch 10/30\n",
      "59/59 [==============================] - 0s 716us/sample - loss: 0.0471 - accuracy: 0.4853 - val_loss: 0.0379 - val_accuracy: 0.4896\n",
      "Epoch 11/30\n",
      "59/59 [==============================] - 0s 835us/sample - loss: 0.0471 - accuracy: 0.4852 - val_loss: 0.0379 - val_accuracy: 0.4896\n",
      "Epoch 12/30\n",
      "59/59 [==============================] - 0s 682us/sample - loss: 0.0463 - accuracy: 0.4852 - val_loss: 0.0379 - val_accuracy: 0.4896\n",
      "Epoch 13/30\n",
      "59/59 [==============================] - 0s 689us/sample - loss: 0.0465 - accuracy: 0.4852 - val_loss: 0.0380 - val_accuracy: 0.4896\n",
      "Epoch 14/30\n",
      "59/59 [==============================] - 0s 700us/sample - loss: 0.0468 - accuracy: 0.4853 - val_loss: 0.0380 - val_accuracy: 0.4896\n",
      "Epoch 15/30\n",
      "59/59 [==============================] - 0s 749us/sample - loss: 0.0461 - accuracy: 0.4853 - val_loss: 0.0380 - val_accuracy: 0.4896\n",
      "Epoch 16/30\n",
      "59/59 [==============================] - 0s 758us/sample - loss: 0.0460 - accuracy: 0.4853 - val_loss: 0.0380 - val_accuracy: 0.4896\n",
      "Epoch 17/30\n",
      "59/59 [==============================] - 0s 770us/sample - loss: 0.0461 - accuracy: 0.4853 - val_loss: 0.0379 - val_accuracy: 0.4896\n",
      "Epoch 18/30\n",
      "59/59 [==============================] - 0s 814us/sample - loss: 0.0475 - accuracy: 0.4853 - val_loss: 0.0378 - val_accuracy: 0.4896\n",
      "Epoch 19/30\n",
      "59/59 [==============================] - 0s 716us/sample - loss: 0.0467 - accuracy: 0.4852 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 20/30\n",
      "59/59 [==============================] - 0s 717us/sample - loss: 0.0459 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 21/30\n",
      "59/59 [==============================] - 0s 749us/sample - loss: 0.0459 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 22/30\n",
      "59/59 [==============================] - 0s 750us/sample - loss: 0.0453 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 23/30\n",
      "59/59 [==============================] - 0s 725us/sample - loss: 0.0463 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 24/30\n",
      "59/59 [==============================] - 0s 743us/sample - loss: 0.0451 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 25/30\n",
      "59/59 [==============================] - 0s 752us/sample - loss: 0.0455 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 26/30\n",
      "59/59 [==============================] - 0s 654us/sample - loss: 0.0453 - accuracy: 0.4853 - val_loss: 0.0377 - val_accuracy: 0.4896\n",
      "Epoch 27/30\n",
      "59/59 [==============================] - 0s 838us/sample - loss: 0.0455 - accuracy: 0.4852 - val_loss: 0.0376 - val_accuracy: 0.4896\n",
      "Epoch 28/30\n",
      "59/59 [==============================] - 0s 760us/sample - loss: 0.0444 - accuracy: 0.4853 - val_loss: 0.0376 - val_accuracy: 0.4896\n",
      "Epoch 29/30\n",
      "59/59 [==============================] - 0s 764us/sample - loss: 0.0447 - accuracy: 0.4853 - val_loss: 0.0376 - val_accuracy: 0.4896\n",
      "Epoch 30/30\n",
      "59/59 [==============================] - 0s 749us/sample - loss: 0.0454 - accuracy: 0.4853 - val_loss: 0.0376 - val_accuracy: 0.4896\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "# too few data to have a batch_size\n",
    "batch_size = 5\n",
    "num_epoch = 30\n",
    "#model training\n",
    "model_log = model.fit(train_features, train_labels,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 313us/sample - loss: 0.0394 - accuracy: 0.4853\n",
      "[0.0406748512917656, 0.4853396]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(train_features, train_labels, verbose=1)\n",
    "print(score)\n",
    "# print('Train loss:', score[0]) \n",
    "# print('Train accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1033814 , 0.19981924, 0.07360041, ..., 0.00245315, 0.00193462,\n",
       "        0.00180954],\n",
       "       [0.10080591, 0.15980485, 0.06307247, ..., 0.00163779, 0.00133416,\n",
       "        0.00131047],\n",
       "       [0.10593888, 0.20870295, 0.07183036, ..., 0.00168559, 0.00161546,\n",
       "        0.00134528],\n",
       "       ...,\n",
       "       [0.13514945, 0.26793402, 0.11710921, ..., 0.0073657 , 0.00680143,\n",
       "        0.00610012],\n",
       "       [0.09436324, 0.14985222, 0.05496398, ..., 0.00113788, 0.00103033,\n",
       "        0.00079548],\n",
       "       [0.08890319, 0.12798741, 0.05039504, ..., 0.00094579, 0.00078535,\n",
       "        0.00067958]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "model.predict(train_features)\n",
    "test_predicted = model.predict(test_features)\n",
    "test_predicted\n",
    "# test_predicted = np.exp(test_predicted)\n",
    "# # test_softmax_sum = np.sum(test_predicted, axis = 1, keepdims=True)\n",
    "# # test_predicted = test_predicted / test_softmax_sum\n",
    "# test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/1 [==================================================================================================================================================================================================================] - 0s 600us/sample - loss: 0.0376 - accuracy: 0.4896\n",
      "[0.03755860775709152, 0.4895725]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_features, test_labels, verbose=1)\n",
    "print(score)\n",
    "# print('Test loss:', score[0]) \n",
    "# print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model\n",
    "# ks = [1, 2, 3, 4, 5, 7, 9, 10, 15, 25]\n",
    "# for k in ks:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn.fit(train_features, train_labels)\n",
    "#     cur_score = np.mean(cross_val_score(knn, train_features, train_labels, cv=5))\n",
    "\n",
    "#     if cur_score > max_score:\n",
    "#         max_score = cur_score\n",
    "#         max_k = k\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "### KNN Prediction part ###\n",
    "### Get data ###\n",
    "with open('random_playlists.json') as json_file:\n",
    "    random_playlists = json.load(json_file)\n",
    "\n",
    "print(len(random_playlists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1276, 5987, 2690, 3677, 6878, 5240, 2816, 3322, 4631, 1117]\n"
     ]
    }
   ],
   "source": [
    "# random.seed(209)\n",
    "playlist_indices = random.sample(range(0, 10000), 10)\n",
    "print(playlist_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomely select 10 playlists out of 10000. \n",
    "random_playlists_for_recomd = []\n",
    "for ind in playlist_indices:\n",
    "    random_playlists_for_recomd.append(random_playlists[ind])\n",
    "# print (random_playlists_for_recomd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n"
     ]
    }
   ],
   "source": [
    "# extract songs from the playlists\n",
    "# result random_songs is a list of songs (each element is a pair of artist name and track name)\n",
    "random_songs = []\n",
    "for playlist in random_playlists_for_recomd:\n",
    "    artists = list(playlist[\"artist_name\"].values())\n",
    "    tracks = list(playlist[\"track_name\"].values())\n",
    "    song_pairs = list(zip(artists, tracks))\n",
    "    random_songs = random_songs + song_pairs\n",
    "    \n",
    "print(len(random_songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(songs_with_tags.keys())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get tags for all randomly selected candidate songs\n",
    "# API_KEY = \"caba40286bcee1b73988cac3d1d408e6\"\n",
    "# API_SECRET = \"297babeacf457cb7d2bf3bd1584e54ea\"\n",
    "# password_hash = \"123456!abcd\"\n",
    "# username = \"bootownine\"\n",
    "# network = pylast.LastFMNetwork(api_key=API_KEY, api_secret=API_SECRET,\n",
    "#                                username=username, password_hash=password_hash)\n",
    "\n",
    "# songs_with_tags = {}\n",
    "\n",
    "# #unique tags threshold\n",
    "# threshold = 3000\n",
    "# #threshold = 5000\n",
    "# unique_tags = []\n",
    "# with open(\"lastfm_unique_tags.txt\", 'r') as f:\n",
    "#     for line in f:\n",
    "#         tag_freq = line.split('\\t')\n",
    "#         if int(tag_freq[1]) > threshold:\n",
    "#             unique_tags.append(tag_freq[0])\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "# def get_zero_dict():\n",
    "#     zeros_dict = {}\n",
    "#     for tag in unique_tags:\n",
    "#         zeros_dict.update({tag: 0.0})\n",
    "#     return zeros_dict\n",
    "\n",
    "# def get_tags_from_track(artist_name, track_name):\n",
    "#     try:\n",
    "#         track = network.get_track(artist_name, track_name)\n",
    "#         tags = track.get_top_tags()\n",
    "#         tags_with_weights = []\n",
    "#         for tag_item in tags:\n",
    "#             if str(tag_item.item) in unique_tags:\n",
    "#                 tags_with_weights.append((str(tag_item.item), int(tag_item.weight)))\n",
    "#         return tags_with_weights\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# for track in random_songs:\n",
    "#     tag_dict = get_zero_dict()\n",
    "#     tags_with_weights = get_tags_from_track(track[0], track[1])\n",
    "#     if tags_with_weights is not None and len(tags_with_weights) != 0:\n",
    "#         for pair in tags_with_weights:\n",
    "#             tag_dict.update({pair[0]: pair[1]/100.0})\n",
    "# #     print(tag_dict)\n",
    "#     songs_with_tags.update({(track[0], track[1]): list(tag_dict.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_with_tags = []\n",
    "with open('candidates_with_tags.json') as json_file:\n",
    "    candidates_with_tags = json.load(json_file)\n",
    "#     print(candidates_with_tags.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: tags for candidate songs, and the predicted tags of one playlist in the test dataset from a model\n",
    "# OUTPUT: a song recommendation\n",
    "def make_recommendation(candidate_tags, playlist_predicted_tag):    \n",
    "    min_distance = float('inf');\n",
    "    rec = -1;\n",
    "    \n",
    "    candidate_songs = list(candidate_tags.keys())\n",
    "    for c in range(len(candidate_songs)):\n",
    "        key = candidate_songs[c]\n",
    "        candidate = candidate_tags[key]\n",
    "        each_distance = np.sum((np.array(candidate) - np.array(playlist_predicted_tag)) ** 2)\n",
    "        if each_distance < min_distance:\n",
    "            min_distance = each_distance;\n",
    "#             print(min_distance)\n",
    "            rec = key;\n",
    "    return rec;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict by KNN ### \n",
    "candidate_tags = candidates_with_tags\n",
    "recommendations = []\n",
    "for each_playlist_predicted_tag in test_predicted:\n",
    "    rec_for_test = make_recommendation(candidate_tags, each_playlist_predicted_tag)\n",
    "    recommendations.append(rec_for_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Disclosure*,*Omen - Radio Edit', 'Disclosure*,*Omen - Radio Edit', 'Disclosure*,*Omen - Radio Edit', 'Disclosure*,*Omen - Radio Edit', 'Ester Dean*,*Drop It Low', 'Disclosure*,*Omen - Radio Edit', 'Disclosure*,*Omen - Radio Edit']\n"
     ]
    }
   ],
   "source": [
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs_recommended = [];\n",
    "# for rec in recommendations:\n",
    "#     songs_recommended.append(random_songs[rec])\n",
    "# print(songs_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
